{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-04-05T01:39:48.730904Z",
     "start_time": "2024-04-05T01:39:48.037695Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c1471b7a336aabf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T01:39:51.468516Z",
     "start_time": "2024-04-05T01:39:51.451561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the 10x10 grid map\n",
    "# note that (0,0) is in the bottom-left corner\n",
    "# the map is stored as a 1D list\n",
    "# the cell (x,y) is indexed as [x+y*10] for x in [0,9] and y in [0,9]\n",
    "# x is the horizontal axis, y is the vertical axis\n",
    "# so the index looks like: \n",
    "#\n",
    "# 90 91 92 93 94 95 96 97 98 99\n",
    "# 80 81 82 83 84 85 86 87 88 89\n",
    "# ...\n",
    "# 10 11 12 13 14 15 16 17 18 19\n",
    "# 00 01 02 03 04 05 06 07 08 09\n",
    "#\n",
    "# each element is a number storing the reward for ending up in that cell\n",
    "\n",
    "# Create the map\n",
    "state_reward_map = np.zeros(10 ** 2, dtype=int)\n",
    "\n",
    "# Abot special grids: \n",
    "# If the agent enters an obstacle cell, it will always stay there, won't be able to move\n",
    "# If the agent enters a goal cell, it will always stay and receive the goal state reward at every time step\n",
    "\n",
    "# Define the rewards of special grids\n",
    "# The obstacle cells are [[9, 9], [8, 9], [7, 9], [6, 9], [5, 9], [4, 9], [3, 9], [2, 9], [1, 9], [0, 9],\n",
    "# [9, 8], [9, 7], [9, 6], [9, 5], [9, 4], [9, 3], [9, 2], [9, 1], [9, 0],\n",
    "# [0, 0], [1, 0], [2, 0], [3, 0], [4, 0], [5, 0], [6, 0], [7, 0], [8, 0], [9, 0],\n",
    "# [0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [0, 9],\n",
    "# [3, 2], [4, 2], [5, 2], [6, 2],\n",
    "# [4, 4], [4, 5], [4, 6], [4, 7], [5, 7],\n",
    "# [7, 4], [7, 5]]\n",
    "# Ending up in an obstacle cell receives a reward of -10 every time step\n",
    "obstacle_coords = [[9, 9], [8, 9], [7, 9], [6, 9], [5, 9], [4, 9], [3, 9], [2, 9], [1, 9], [0, 9],\n",
    "                  [9, 8], [9, 7], [9, 6], [9, 5], [9, 4], [9, 3], [9, 2], [9, 1], [9, 0],\n",
    "                  [0, 0], [1, 0], [2, 0], [3, 0], [4, 0], [5, 0], [6, 0], [7, 0], [8, 0], [9, 0],\n",
    "                  [0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [0, 9],\n",
    "                  [3, 2], [4, 2], [5, 2], [6, 2],\n",
    "                  [4, 4], [4, 5], [4, 6], [4, 7], [5, 7],\n",
    "                  [7, 4], [7, 5]]\n",
    "for cell in obstacle_coords:\n",
    "    state_reward_map[cell[0] + cell[1] * 10] = -10\n",
    "\n",
    "# Define the goal cells\n",
    "goal_coords = [[8, 1]]\n",
    "# Ending up in a goal cell receives a reward of 10 every time step\n",
    "for cell in goal_coords:\n",
    "    state_reward_map[cell[0] + cell[1] * 10] = 10\n",
    "    \n",
    "# Define the origin cells\n",
    "origin_1_coords = [[1, 1]]\n",
    "origin_2_coords = [[3, 6]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8953bdf4d511e4d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T01:39:53.256650Z",
     "start_time": "2024-04-05T01:39:53.057584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reward map:\n",
      "[-10 -10 -10 -10 -10 -10 -10 -10 -10 -10]\n",
      "[-10   0   0   0   0   0   0   0  10 -10]\n",
      "[-10   0   0 -10 -10 -10 -10   0   0 -10]\n",
      "[-10   0   0   0   0   0   0   0   0 -10]\n",
      "[-10   0   0   0 -10   0   0 -10   0 -10]\n",
      "[-10   0   0   0 -10   0   0 -10   0 -10]\n",
      "[-10   0   0   0 -10   0   0   0   0 -10]\n",
      "[-10   0   0   0 -10 -10   0   0   0 -10]\n",
      "[-10   0   0   0   0   0   0   0   0 -10]\n",
      "[-10 -10 -10 -10 -10 -10 -10 -10 -10 -10]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 500x500 with 2 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbcAAAGiCAYAAABkjIjDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8WgzjOAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA1RUlEQVR4nO3de1RVdf7/8deBBCQ5eEMuiop2QUVTURlwvJRM6NjFpuWU2XjJscuAZlgTtkxLM7KrfdO8zLegmTRtvpPptMp+hvcRb5iVjlk2KmSB2iQorkA5+/dH45lOgnrY53A25zwfa+21PB/2Z+/3obV4937vz97bZhiGIQAA/EiQrwMAAMDTSG4AAL9DcgMA+B2SGwDA75DcAAB+h+QGAPA7JDcAgN8huQEA/A7JDQDgd0huAAC/Q3IDADht2rRJN998s+Li4mSz2fTuu++6/NwwDM2YMUOxsbFq2rSp0tPT9eWXX17yuAsWLFDHjh0VFhamlJQU7dixw0vf4EckNwCAU2Vlpa677jotWLCg1p8/++yz+p//+R8tWrRI27dv15VXXqmMjAz98MMPdR5zxYoVys7O1syZM7V7925dd911ysjI0LFjx7z1NWTjwckAgNrYbDatXLlSI0aMkPRj1RYXF6epU6fq4YcfliSVl5crOjpa+fn5uvPOO2s9TkpKivr27av58+dLkhwOh+Lj4zVp0iTl5OR4JfYrvHJUAEC9/fDDD6qurvbIsQzDkM1mcxkLDQ1VaGio28c6dOiQSktLlZ6e7hyLjIxUSkqKCgsLa01u1dXVKioq0rRp05xjQUFBSk9PV2FhodsxXC6SGwBYyA8//KCEDnaVHjvrkeM1a9ZMp0+fdhmbOXOmnnjiCbePVVpaKkmKjo52GY+Ojnb+7OdOnDihmpqaWud8/vnnbsdwuUhuAGAh1dXVKj12VsW7esoeEWzqWBWnatS+zx6VlJTIbrc7x+tTtTU2JDcAsCB7RLDp5OY8lt3uktzqKyYmRpJUVlam2NhY53hZWZl69uxZ65zWrVsrODhYZWVlLuNlZWXO43kDqyUBwIoMwzObByUkJCgmJkYFBQXOsYqKCm3fvl2pqam1zgkJCVFycrLLHIfDoYKCgjrneAKVGwBYkSeSUz3mnz59WgcPHnR+PnTokPbs2aOWLVuqffv2mjJlip566ildffXVSkhI0OOPP664uDjnikpJGjJkiG677TZlZWVJkrKzszV27Fj16dNH/fr107x581RZWanx48eb+34XQXIDADjt2rVL119/vfNzdna2JGns2LHKz8/XH//4R1VWVuree+/VyZMn9ctf/lJr1qxRWFiYc85XX32lEydOOD/fcccdOn78uGbMmKHS0lL17NlTa9asuWCRiSdxnxsAWEhFRYUiIyP1/T97eWRBSYuuH6u8vNwj19waEyo3ALAgwwiSYZhbFhHItQsLSgAAfofKDQAsyDBsHqjcHB6KpvEhuQGABTmMIDlMJjez8xuzwP3mAAC/ReUGABbkmQUlgVu/kNwAwIJIbuYE7jcHAPgtKjcAsKAfV0vaLr3jJY4RqKjcAtiGDRtks9n0f//3f74OpdE5fPiwbDab8vPzfR0K/NT5tqTZLVBRufmZn79xty7r16/3ciQA4DskNz/zl7/8xeXzn//8Z61du/aC8S5dumj//v0NGRoANzgMmxwm24pm5zdmJDc/c/fdd7t83rZtm9auXXvBuCSfJrcffvhBISEhCgqybtvkzJkzCg8P93UYCFCGPLBaMoCvPAXuN4eTw+HQnDlz1K5dO4WFhWnIkCEu73M6b/v27Ro6dKgiIyMVHh6uQYMG6R//+Mclj3/+2t7y5cs1ffp0tW3bVuHh4aqoqLis43766aey2WxavXq1c6yoqEg2m029e/d2OdewYcOUkpLi/Lxq1SoNHz5ccXFxCg0NVefOnTV79mzV1NS4zBs8eLCSkpJUVFSkgQMHKjw8XI899pgk6eTJkxo3bpwiIyPVvHlzjR07VidPnrz0L1ZSfn6+bDabtmzZosmTJysqKkrNmzfXfffdp+rqap08eVJjxoxRixYt1KJFC/3xj3+84GG3zz//vNLS0tSqVSs1bdpUycnJtV4ntdlsysrK0tKlS3XttdcqLCxMycnJ2rRp02XFCvgTKjfomWeeUVBQkB5++GGVl5fr2Wef1ejRo7V9+3bnPuvWrdOwYcOUnJysmTNnKigoSHl5ebrhhhu0efNm9evX75LnmT17tkJCQvTwww+rqqpKISEhl3XcpKQkNW/eXJs2bdItt9wiSdq8ebOCgoL0ySefqKKiQna7XQ6HQ1u3btW9997rPGd+fr6aNWum7OxsNWvWTOvWrdOMGTNUUVGh5557ziW+7777TsOGDdOdd96pu+++W9HR0TIMQ7feequ2bNmi+++/X126dNHKlSs1duxYt37HkyZNUkxMjJ588klt27ZNS5YsUfPmzbV161a1b99eTz/9tN5//30999xzSkpK0pgxY5xzX375Zd1yyy0aPXq0qqurtXz5co0cOVLvvfeehg8f7nKejRs3asWKFZo8ebJCQ0P16quvaujQodqxY4eSkpLcihm+xWpJkwz4tczMTKOu/8zr1683JBldunQxqqqqnOMvv/yyIcn47LPPDMMwDIfDYVx99dVGRkaG4XA4nPudOXPGSEhIMH71q19dNIbz5+nUqZNx5swZ57g7xx0+fLjRr18/5+ff/OY3xm9+8xsjODjY+OCDDwzDMIzdu3cbkoxVq1a5HOvn7rvvPiM8PNz44YcfnGODBg0yJBmLFi1y2ffdd981JBnPPvusc+zcuXPGgAEDDElGXl7eRb97Xl6eIemC75iammrYbDbj/vvvdzluu3btjEGDBrkc4+ffobq62khKSjJuuOEGl3FJhiRj165dzrEjR44YYWFhxm233XbROGEd5eXlhiSjeNcNxsnPbzS1Fe+6wZBklJeX+/prNTjaktD48eMVEhLi/DxgwABJ0r/+9S9J0p49e/Tll1/qrrvu0nfffacTJ07oxIkTqqys1JAhQ7Rp0yY5HJd++vjYsWPVtGlT52d3jjtgwADt3r1blZWVkqQtW7bo17/+tXr27KnNmzdL+rGas9ls+uUvf+k8x0/Pd+rUKZ04cUIDBgzQmTNn9Pnnn7vEFxoaesFr799//31dccUVeuCBB5xjwcHBmjRp0iW/709NmDDBZSVrSkqKDMPQhAkTXI7bp08f5++9tu/w/fffq7y83Pn7+LnU1FQlJyc7P7dv31633nqrPvzwwwtasYA/oy0JtW/f3uVzixYtJP34h1SSvvzyS0m6aCuuvLzcOa8uCQkJLp/dOe6AAQN07tw5FRYWKj4+XseOHdOAAQO0b98+l+TWtWtXtWzZ0jl/3759mj59utatW+e8xvfTY/9U27ZtXZK8JB05ckSxsbFq1qyZy/i111570e/6cz//HUdGRkqS4uPjLxg//3s/77333tNTTz2lPXv2qKqqyjle220fV1999QVj11xzjc6cOaPjx48rJibGrbjhOzx+yxySGxQcXPur7I3/LGw4Xz0999xz6tmzZ637/vyPf21+WoG4e9w+ffooLCxMmzZtUvv27dWmTRtdc801GjBggF599VVVVVVp8+bNuu2225xzT548qUGDBslut2vWrFnq3LmzwsLCtHv3bj366KMXVJs/j8+T6vod1zZu/GRByebNm3XLLbdo4MCBevXVVxUbG6smTZooLy9Py5Yt81q88D3DMH/NLIBfxE1yw6V17txZkmS325Wenu6T44aEhKhfv37avHmz2rdv72ydDhgwQFVVVVq6dKnKyso0cOBA55wNGzbou+++0zvvvOMyfujQocuOsUOHDiooKNDp06ddEviBAwcu+xhm/O1vf1NYWJg+/PBDhYaGOsfz8vJq3f98NfxTX3zxhcLDwxUVFeW1OAGrCdyaFZctOTlZnTt31vPPP6/Tp09f8PPjx483yHEHDBig7du3a/369c7k1rp1a3Xp0kVz58517nPe+arop5VQdXW1Xn311cuO8de//rXOnTunhQsXOsdqamr0yiuvXPYxzAgODpbNZnO5Xnb48GG9++67te5fWFjoci2upKREq1at0o033lhn9QhrOr9a0uwWqKjccElBQUH63//9Xw0bNkzdunXT+PHj1bZtWx09elTr16+X3W7X3//+d68fd8CAAZozZ45KSkpcktjAgQO1ePFidezYUe3atXOOp6WlqUWLFho7dqwmT54sm82mv/zlLxfcR3YxN998s/r376+cnBwdPnxYXbt21TvvvHPB9TpvGT58uF588UUNHTpUd911l44dO6YFCxboqquu0qeffnrB/klJScrIyHC5FUCSnnzyyQaJF55jeOAJJSQ34BIGDx6swsJCzZ49W/Pnz9fp06cVExOjlJQU3XfffQ1y3LS0NAUHBys8PFzXXXedc3zAgAFavHixS8KTpFatWum9997T1KlTNX36dLVo0UJ33323hgwZooyMjMuKLygoSKtXr9aUKVP05ptvymaz6ZZbbtELL7ygXr161ft7X64bbrhBr732mp555hlNmTJFCQkJmjt3rg4fPlxrchs0aJBSU1P15JNPqri4WF27dlV+fr569Ojh9VgBK7EZ7vxvLADLstlsyszM1Pz5830dCkyoqKhQZGSkvirMUESzJqaOder0WXVO/VDl5eWy2+0eirBxoHIDAAsyFGT62ZA8WxIAAD9C5QYAFsSzJc0huQF+gsvn/oXkZg5tSQCA32nwys3hcOibb75RRERErc/GA4DGxjAMnTp1SnFxcR57AS+VmzkNnty++eabCx4WCwD+oKSkxOVBAmaQ3Mxp8OQWERHx4z/ap0tBXPID4Acc56Tij/779w0+1+DZxdmKDLpCCjJ3gyIAWIknL7VQuZlD6QQAFuTwwLMlzc5vzFgtCQDwO1RuAGBBtCXNIbkBgAWR3MyhLQkAkCR17NhRNpvtgi0zM7PW/fPz8y/YNywsrIGjrh2VGwBYkC8qt507d7q89X3v3r361a9+pZEjR9Y5x26368CBA87PVnk4B8kNACzIF8ktKirK5fMzzzyjzp07a9CgQXXOsdlsiomJqVd83kRbEgD8XEVFhctWVVV1yTnV1dV68803dc8991y0Gjt9+rQ6dOig+Ph43Xrrrdq3b58nQ683khsAWJAhyTBMbv85Vnx8vCIjI51bbm7uJc//7rvv6uTJkxo3blyd+1x77bV6/fXXtWrVKr355ptyOBxKS0vT119/7ZHfgRm0JQHAggzZZMhkW/I/80tKSmS3253joaGhl5z72muvadiwYYqLi6tzn9TUVKWmpjo/p6WlqUuXLlq8eLFmz55tInLzSG4A4OfsdrtLcruUI0eO6KOPPtI777zj1nmaNGmiXr166eDBg+6G6HG0JQHAgs4vKDG71UdeXp7atGmj4cOHuzWvpqZGn332mWJjY+t1Xk+icgMAK/LAaknVY77D4VBeXp7Gjh2rK65wTRFjxoxR27ZtndfsZs2apV/84he66qqrdPLkST333HM6cuSIfv/735uL2wNIbgAAp48++kjFxcW65557LvhZcXGxy8tYv//+e02cOFGlpaVq0aKFkpOTtXXrVnXt2rUhQ66VzTAM49K7eU5FRYUiIyOljkN55Q0A/+A4Kx1eo/LycreubdXm/N/Iov/3WzW7MsTUsU5XViv5xrc9EldjQ+UGABbkMH7czB4jULGgBADgd6jcAMCCeCuAOSQ3ALAgkps5tCUBAH6Hyg0ALIjKzRySGwBY0PmHH5s9RqCiLQkA8DtUbgBgQZ58K0AgIrkBgAVxzc0c2pIAAL9D5QYAFkTlZg7JDQAsiNWS5gRccps+qpuvQwDgRU+9tc/XIcACAi65AUBjQFvSHJIbAFgQbUlzWC0JAPA7VG4AYEG0Jc0huQGABZHczKEtCQDwO1RuAGBBjv9sZo8RqEhuAGBFHmhLirYkAAD+g8oNACyIBSXmkNwAwIIMeeAmbo9E0ji51ZasqanR448/roSEBDVt2lSdO3fW7NmzZQTybfAAAMtxq3KbO3euFi5cqDfeeEPdunXTrl27NH78eEVGRmry5MneihEAAg5tSXPcSm5bt27VrbfequHDh0uSOnbsqLfeeks7duzwSnAAEKh4tqQ5brUl09LSVFBQoC+++EKS9Mknn2jLli0aNmxYnXOqqqpUUVHhsgEA4E1uVW45OTmqqKhQYmKigoODVVNTozlz5mj06NF1zsnNzdWTTz5pOlAACCS0Jc1xq3J7++23tXTpUi1btky7d+/WG2+8oeeff15vvPFGnXOmTZum8vJy51ZSUmI6aADwd4aHtkDlVuX2yCOPKCcnR3feeackqXv37jpy5Ihyc3M1duzYWueEhoYqNDTUfKQAAFwmt5LbmTNnFBTkWuwFBwfL4QjkJ5gBgOfRljTHreR28803a86cOWrfvr26deumjz/+WC+++KLuueceb8UHAAGJ1ZLmuJXcXnnlFT3++OP6wx/+oGPHjikuLk733XefZsyY4a34AABwm1vJLSIiQvPmzdO8efO8FA4AQKItaRbPlgQAC6ItaQ6vvAEASJKeeOIJ2Ww2ly0xMfGic/76178qMTFRYWFh6t69u95///0GivbiSG4AYEHn25JmN3d169ZN3377rXPbsmVLnftu3bpVo0aN0oQJE/Txxx9rxIgRGjFihPbu3Wvmq3sEyQ0ALMhXN3FfccUViomJcW6tW7euc9+XX35ZQ4cO1SOPPKIuXbpo9uzZ6t27t+bPn1+PM3sWyQ0A/NzPn+9bVVVV575ffvml4uLi1KlTJ40ePVrFxcV17ltYWKj09HSXsYyMDBUWFnos9voiuQGABXmyLRkfH6/IyEjnlpubW+s5U1JSlJ+frzVr1mjhwoU6dOiQBgwYoFOnTtW6f2lpqaKjo13GoqOjVVpa6tlfRj2wWhIALMiTqyVLSkpkt9ud43U9EvGnb3jp0aOHUlJS1KFDB7399tuaMGGCuWAaGMkNAPyc3W53SW6Xq3nz5rrmmmt08ODBWn8eExOjsrIyl7GysjLFxMTUK05PIrnB6556a5+vQ/CZ6aO6+ToENFJWuM/t9OnT+uqrr/S73/2u1p+npqaqoKBAU6ZMcY6tXbtWqamp5k7sAVxzAwAL+jG5mb3m5t45H374YW3cuFGHDx/W1q1bddtttyk4OFijRo2SJI0ZM0bTpk1z7v/ggw9qzZo1euGFF/T555/riSee0K5du5SVleXJX0W9ULkBACRJX3/9tUaNGqXvvvtOUVFR+uUvf6lt27YpKipKklRcXOzyZpi0tDQtW7ZM06dP12OPPaarr75a7777rpKSknz1FZxIbgBgQb5oSy5fvvyiP9+wYcMFYyNHjtTIkSPdO1EDILkBgCXZZMjsg48D98HJXHMDAPgdKjcAsCArrJZszEhuAGBBJDdzaEsCAPwOlRsAWBBv4jaH5AYAFuQwftzMHiNQ0ZYEAPgdKjcAsCDDA/e5mb9PrvEiuQGABbFa0hzakgAAv0PlBgAWZMgDlZtHImmcSG4AYEHcCmAObUkAgN+hcgMAC2JBiTkkNwCwIJKbObQlAQB+h8oNACyIm7jNIbkBgAXRljSHtiQAwO9QuQGABVG5mUNyAwALIrmZQ1sSAOB3qNwAwIJ4/JY5JDcAsCBD5h98HMBdSdqSAAD/Q+UGABbEghJzSG7wa9NHdfN1CED9eCC5BXJfkrYkAMDvULkBgAWxWtIckhsAWBCrJc2hLQkA8DtUbgBgQayWNIfkBgAWRHIzh7YkAMDvULkBgAWxWtIckhsAWBBtSXNoSwIAJEm5ubnq27evIiIi1KZNG40YMUIHDhy46Jz8/HzZbDaXLSwsrIEirhvJDQAsyPDQ5o6NGzcqMzNT27Zt09q1a3X27FndeOONqqysvOg8u92ub7/91rkdOXLEzTN7ntttyaNHj+rRRx/VBx98oDNnzuiqq65SXl6e+vTp4434ACAg+aItuWbNGpfP+fn5atOmjYqKijRw4MA659lsNsXExNQnRK9xK7l9//336t+/v66//np98MEHioqK0pdffqkWLVp4Kz4AgEkVFRUun0NDQxUaGnrJeeXl5ZKkli1bXnS/06dPq0OHDnI4HOrdu7eefvppdevm24eWu5Xc5s6dq/j4eOXl5TnHEhISPB4UAAQ6T1Zu8fHxLuMzZ87UE088cdG5DodDU6ZMUf/+/ZWUlFTnftdee61ef/119ejRQ+Xl5Xr++eeVlpamffv2qV27dua+gAluJbfVq1crIyNDI0eO1MaNG9W2bVv94Q9/0MSJE+ucU1VVpaqqKufnn/8fBADgQp68FaCkpER2u905fjlVW2Zmpvbu3astW7ZcdL/U1FSlpqY6P6elpalLly5avHixZs+eXc/IzXNrQcm//vUvLVy4UFdffbU+/PBDPfDAA5o8ebLeeOONOufk5uYqMjLSuf38/yAAAN5lt9tdtkslt6ysLL333ntav36929VXkyZN1KtXLx08eNBMyKa5ldx+2k/t1auX7r33Xk2cOFGLFi2qc860adNUXl7u3EpKSkwHDQD+zherJQ3DUFZWllauXKl169bV67JTTU2NPvvsM8XGxro915PcakvGxsaqa9euLmNdunTR3/72tzrnXO6FSwDAfxnywDU3N/fPzMzUsmXLtGrVKkVERKi0tFSSFBkZqaZNm0qSxowZo7Zt2yo3N1eSNGvWLP3iF7/QVVddpZMnT+q5557TkSNH9Pvf/95c8Ca5ldz69+9/wQ19X3zxhTp06ODRoAAADW/hwoWSpMGDB7uM5+Xlady4cZKk4uJiBQX9t+n3/fffa+LEiSotLVWLFi2UnJysrVu3XlAINTS3kttDDz2ktLQ0Pf300/rtb3+rHTt2aMmSJVqyZIm34gOAgOSL+9yMy5iwYcMGl88vvfSSXnrpJfdO1ADcuubWt29frVy5Um+99ZaSkpI0e/ZszZs3T6NHj/ZWfAAQkM4nN7NboHL7CSU33XSTbrrpJm/EAgCAR/BWAACwIN4KYA7JDQAsyJBNhkzexG1yfmPGWwEAAH6Hyg0ALIi2pDkkNwCwovo8YqS2YwQo2pIAAL9D5QYAVuSJ+9QCuHIjuQGABdGVNIfkBvipp97a57NzTx/l27cwAyQ3ALAgVkuaQ3IDAAsiuZnDakkAgN+hcgMAC6JyM4fkBgAWxGpJc2hLAgD8DpUbAFgQbUlzSG4AYEEkN3NoSwIA/A6VGwBYEJWbOSQ3ALAgVkuaQ1sSAOB3qNwAwIJoS5pDcgMACzIMmwzDZvoYgYq2JADA71C5AYAF0ZY0h+QGABbEaklzaEsCAPwOlRsAWBBtSXNIbgBgQSQ3c2hLAgD8DpUbAFiQIUOGydLLCOAlJSQ3ALAgVkuaQ1sSAOB3qNwAwIo8sKAkkEs3khsAWBCrJc2hLQkAcLFgwQJ17NhRYWFhSklJ0Y4dOy66/1//+lclJiYqLCxM3bt31/vvv99AkdaN5AYAVmR4aHPTihUrlJ2drZkzZ2r37t267rrrlJGRoWPHjtW6/9atWzVq1ChNmDBBH3/8sUaMGKERI0Zo79697p/cg0huAGBBPsptevHFFzVx4kSNHz9eXbt21aJFixQeHq7XX3+91v1ffvllDR06VI888oi6dOmi2bNnq3fv3po/f349zu45JDcA8HMVFRUuW1VVVa37VVdXq6ioSOnp6c6xoKAgpaenq7CwsNY5hYWFLvtLUkZGRp37NxSSGwBY0PkFJWY3SYqPj1dkZKRzy83NrfWcJ06cUE1NjaKjo13Go6OjVVpaWuuc0tJSt/ZvKKyWBAA/V1JSIrvd7vwcGhrqw2gaBskNACzIk7cC2O12l+RWl9atWys4OFhlZWUu42VlZYqJial1TkxMjFv7NxTakgBgQb5YUBISEqLk5GQVFBQ4xxwOhwoKCpSamlrrnNTUVJf9JWnt2rV17t9QqNwAAE7Z2dkaO3as+vTpo379+mnevHmqrKzU+PHjJUljxoxR27ZtndftHnzwQQ0aNEgvvPCChg8fruXLl2vXrl1asmSJL78GyQ0ArMgwPPBWgHrMv+OOO3T8+HHNmDFDpaWl6tmzp9asWeNcNFJcXKygoP82/dLS0rRs2TJNnz5djz32mK6++mq9++67SkpKMhW7WSQ3ALAgXz5+KysrS1lZWbX+bMOGDReMjRw5UiNHjqzfybyEa24AAL9D5QYAFsSDk80huQGAJfG6UjNoSwIA/A6VGwBYEG1Jc0xVbs8884xsNpumTJnioXAAANJ/bwUwuwWqeie3nTt3avHixerRo4cn4wEAwLR6JbfTp09r9OjR+tOf/qQWLVp4OiYACHiefCtAIKpXcsvMzNTw4cMveIdPbaqqqi54lxAA4OJ89bJSf+H2gpLly5dr9+7d2rlz52Xtn5ubqyeffNLtwAAAqC+3KreSkhI9+OCDWrp0qcLCwi5rzrRp01ReXu7cSkpK6hUoAAQSFpSY41blVlRUpGPHjql3797OsZqaGm3atEnz589XVVWVgoODXeaEhoYGxIvxAMCjuIfbFLeS25AhQ/TZZ5+5jI0fP16JiYl69NFHL0hsAAD4glvJLSIi4oLXGFx55ZVq1aqVz19vAAD+hMLNHJ5QAgAW5Kv3ufkL08mttnf7AADgS1RuAGBBPFvSHJIbAFgQyc0cXnkDAPA7VG4AYEmGDNZL1hvJDQAsiLakObQlAQB+h8oNAKwqgCsvs0huAGBBPKHEHJIbvG76qG6+DiEg8XtHICO5AYAF8fgtc0huAGBBrJY0h9WSAAC/Q+UGABZE5WYOyQ0ALIjVkubQlgQA+B0qNwCwIFZLmkNyAwAL4pqbObQlAQB+h+QGAPA7tCUBwIJoS5pD5QYA8DtUbgBgQVRu5lC5AYAFnb8VwOzmDYcPH9aECROUkJCgpk2bqnPnzpo5c6aqq6svOm/w4MGy2Wwu2/333++VGKncAABu+fzzz+VwOLR48WJdddVV2rt3ryZOnKjKyko9//zzF507ceJEzZo1y/k5PDzcKzGS3ADAgjz5+K2KigqX8dDQUIWGhtb7uEOHDtXQoUOdnzt16qQDBw5o4cKFl0xu4eHhiomJqfe5LxdtSQCwoPPX3MxukhQfH6/IyEjnlpub6/F4y8vL1bJly0vut3TpUrVu3VpJSUmaNm2azpw54/FYJCo3APB7JSUlstvtzs9mqrbaHDx4UK+88solq7a77rpLHTp0UFxcnD799FM9+uijOnDggN555x2PxiOR3ADAkjy5WtJut7skt7rk5ORo7ty5F91n//79SkxMdH4+evSohg4dqpEjR2rixIkXnXvvvfc6/929e3fFxsZqyJAh+uqrr9S5c+dLxucOkhsAWJAvXnkzdepUjRs37qL7dOrUyfnvb775Rtdff73S0tK0ZMkSt+NLSUmR9GPlR3IDAHhFVFSUoqKiLmvfo0eP6vrrr1dycrLy8vIUFOT+Eo49e/ZIkmJjY92eeyksKAEAK/LkihIPO3r0qAYPHqz27dvr+eef1/Hjx1VaWqrS0lKXfRITE7Vjxw5J0ldffaXZs2erqKhIhw8f1urVqzVmzBgNHDhQPXr08HiMVG4AYEFWfkLJ2rVrdfDgQR08eFDt2rX72Tl/POnZs2d14MAB52rIkJAQffTRR5o3b54qKysVHx+v22+/XdOnT/dKjCQ3AIBbxo0bd8lrcx07dnR5Qkp8fLw2btzo5cj+i+QGABbkiwUl/oTkBgBW5IlLZgGc3VhQAgDwO1RuAGBBVl5Q0hiQ3ADAgkhu5tCWBAD4HSq3APHUW/t8HQIa2PRR3XwdAkz4cbWkudIrgAs3khsAWBFtSXNoSwIA/A6VGwBYEJWbOSQ3ALAgnlBiDm1JAIDfoXIDAKsK5NLLJJIbAFgQ19zMoS0JAPA7VG4AYEEsKDGH5AYAFkRb0hzakgAAv+NWcsvNzVXfvn0VERGhNm3aaMSIETpw4IC3YgOAgHW+cjO7BSq3ktvGjRuVmZmpbdu2ae3atTp79qxuvPFGVVZWeis+AAhIhmF4ZAtUbl1zW7Nmjcvn/Px8tWnTRkVFRRo4cKBHAwMAoL5MLSgpLy+XJLVs2bLOfaqqqlRVVeX8XFFRYeaUABAQWC1pTr0XlDgcDk2ZMkX9+/dXUlJSnfvl5uYqMjLSucXHx9f3lAAQMLjmZk69k1tmZqb27t2r5cuXX3S/adOmqby83LmVlJTU95QAAFyWerUls7Ky9N5772nTpk1q167dRfcNDQ1VaGhovYIDgEDFfW7muJXcDMPQpEmTtHLlSm3YsEEJCQneigsAAhrX3MxxK7llZmZq2bJlWrVqlSIiIlRaWipJioyMVNOmTb0SIAAA7nLrmtvChQtVXl6uwYMHKzY21rmtWLHCW/EBQEBiQYk5brclAQDexzU3c3i2JADA7/BWAACwIBaUmENyAwAr8sQ1swDObrQlAQB+h8oNACyIBSXmkNwAwIK45mYObUkAgN+hcgMAC6ItaQ7JLUBMH9XN1yEggDyZtd5n537qrTY+O7cnWT25dezYUUeOHHEZy83NVU5OTp1zfvjhB02dOlXLly9XVVWVMjIy9Oqrryo6Otrj8dGWBADUy6xZs/Ttt986t0mTJl10/4ceekh///vf9de//lUbN27UN998o9/85jdeiY3KDQAsyJOVW0VFhcu4p15FFhERoZiYmMvat7y8XK+99pqWLVumG264QZKUl5enLl26aNu2bfrFL35hOp6fonIDAAsyPLRJUnx8vCIjI51bbm6uR2J85pln1KpVK/Xq1UvPPfeczp07V+e+RUVFOnv2rNLT051jiYmJat++vQoLCz0Sz09RuQGAnyspKZHdbnd+9kTVNnnyZPXu3VstW7bU1q1bNW3aNH377bd68cUXa92/tLRUISEhat68uct4dHS08/VpnkRyAwAL8mRb0m63uyS3uuTk5Gju3LkX3Wf//v1KTExUdna2c6xHjx4KCQnRfffdp9zcXI8kT7NIbgBgQb5YLTl16lSNGzfuovt06tSp1vGUlBSdO3dOhw8f1rXXXnvBz2NiYlRdXa2TJ0+6VG9lZWWXfd3OHSQ3AIAkKSoqSlFRUfWau2fPHgUFBalNm9pvxUhOTlaTJk1UUFCg22+/XZJ04MABFRcXKzU1td4x14XkBgAWZOXHbxUWFmr79u26/vrrFRERocLCQj300EO6++671aJFC0nS0aNHNWTIEP35z39Wv379FBkZqQkTJig7O1stW7aU3W7XpEmTlJqa6vGVkhLJDQAsyco3cYeGhmr58uV64oknVFVVpYSEBD300EMu1+HOnj2rAwcO6MyZM86xl156SUFBQbr99ttdbuL2BpIbAMAtvXv31rZt2y66T8eOHWX8LLuGhYVpwYIFWrBggTfDk0RyAwBLsnLl1hiQ3ADAgqx8za0x4AklAAC/Q+UGABZEW9IckhsAWJAhDyQ3j0TSONGWBAD4HSo3ALAgFpSYQ3IDAAvimps5tCUBAH6Hyg0ALMgwJAeVW72R3ADAgmhLmkNbEgDgd6jcAMCCWC1pDskNACzIMGwyDJvpYwQq2pIAAL9D5QYAFsSCEnNIbgBgQVxzM4e2JADA71C5AYAFOQzJZrL0MnsTeGNGcgMAC+KamzkBl9yeemufr0MA/N5Tb7XxdQgIcAGX3ACgMWBBiTkkNwCwIK65mcNqSQCA36FyAwALYkGJOSQ3ALCgH6+5mXy2pGdCaZRoSwIA/A6VGwBYEAtKzCG5AYAFcc3NHNqSAAC/Q+UGABZkGObbioFcuZHcAMCCeEKJOfVqSy5YsEAdO3ZUWFiYUlJStGPHDk/HBQBAvbmd3FasWKHs7GzNnDlTu3fv1nXXXaeMjAwdO3bMG/EBQEByGJ7ZApXbye3FF1/UxIkTNX78eHXt2lWLFi1SeHi4Xn/9dW/EBwAByTBsHtkClVvJrbq6WkVFRUpPT//vAYKClJ6ersLCwlrnVFVVqaKiwmUDAMCb3EpuJ06cUE1NjaKjo13Go6OjVVpaWuuc3NxcRUZGOrf4+Pj6RwsAAcLhoS1Qef0+t2nTpqm8vNy5lZSUePuUANDoWfma24YNG2Sz2Wrddu7cWee8wYMHX7D//fff75UY3boVoHXr1goODlZZWZnLeFlZmWJiYmqdExoaqtDQ0PpHCACwlLS0NH377bcuY48//rgKCgrUp0+fi86dOHGiZs2a5fwcHh7ulRjdSm4hISFKTk5WQUGBRowYIUlyOBwqKChQVlaWN+IDgIDk8MCNbucrt5+vdTBbdISEhLgUNGfPntWqVas0adIk2WwXX8QSHh5eZzHkSW63JbOzs/WnP/1Jb7zxhvbv368HHnhAlZWVGj9+vDfiA4CA5Mm2ZHx8vMvah9zcXI/Gunr1an333XeXlQeWLl2q1q1bKykpSdOmTdOZM2c8Gst5bj+h5I477tDx48c1Y8YMlZaWqmfPnlqzZs0Fi0wAANZQUlIiu93u/OzpS0WvvfaaMjIy1K5du4vud9ddd6lDhw6Ki4vTp59+qkcffVQHDhzQO++849F4pHo+fisrK4s2JAB4kUM2yeTLSh3/mW+3212SW11ycnI0d+7ci+6zf/9+JSYmOj9//fXX+vDDD/X2229f8vj33nuv89/du3dXbGyshgwZoq+++kqdO3e+5Hx38GxJALAgh2T+mpub+0+dOlXjxo276D6dOnVy+ZyXl6dWrVrplltucfNsUkpKiiTp4MGDJDcAgHdERUUpKirqsvc3DEN5eXkaM2aMmjRp4vb59uzZI0mKjY11e+6l8D43ALAgwwOLSbz9ypt169bp0KFD+v3vf3/Bz44eParExETng/W/+uorzZ49W0VFRTp8+LBWr16tMWPGaODAgerRo4fHY6NyAwALqpHZK27ef+XNa6+9prS0NJdrcOedPXtWBw4ccK6GDAkJ0UcffaR58+apsrJS8fHxuv322zV9+nSvxEZyAwDUy7Jly+r8WceOHWX8pHSMj4/Xxo0bGyIsSSQ3ALCkGkOy8SbueiO5AYAFnSO5mdLgyc1ZpjrONfSpAcA7/vP3zAjkbGIxDZ7cTp069eM/ij9q6FMDgFedOnVKkZGRHjlWjWyymVxSYphektJ4NXhyi4uLU0lJiSIiIi75gM2fq6ioUHx8/AWPkvF3fG++dyBozN/bMAydOnVKcXFxHjsmbUlzGjy5BQUFXfL5Y5dyuY+S8Td878DC925cPFWxwTNYUAIAVuSJm7Cp3AAA1uKBF7oFcHZrVI/fCg0N1cyZMwPuzd58b753IAjU7w3vsBmsXQUAy6ioqPjx+l33B6Rgk4m+pkr6bKHKy8sb5XVMM2hLAoAl0ZY0o1G1JQEAuBxUbgBgRYYhGe6+brSWYwQokhsAWJHhgXsBAji50ZYEAPidRpXcFixYoI4dOyosLEwpKSnON7z6q9zcXPXt21cRERFq06aNRowYoQMHDvg6rAb1zDPPyGazacqUKb4OpUEcPXpUd999t1q1aqWmTZuqe/fu2rVrl6/D8qqamho9/vjjSkhIUNOmTdW5c2fNnj2bhxDL4aEtMDWa5LZixQplZ2dr5syZ2r17t6677jplZGTo2LFjvg7NazZu3KjMzExt27ZNa9eu1dmzZ3XjjTeqsrLS16E1iJ07d2rx4sVeeQW9FX3//ffq37+/mjRpog8++ED//Oc/9cILL6hFixa+Ds2r5s6dq4ULF2r+/Pnav3+/5s6dq2effVavvPKKr0PzLcPhmS1ANZr73FJSUtS3b1/Nnz9fkuRwOBQfH69JkyYpJyfHx9E1jOPHj6tNmzbauHGjBg4c6OtwvOr06dPq3bu3Xn31VT311FPq2bOn5s2b5+uwvConJ0f/+Mc/tHnzZl+H0qBuuukmRUdH67XXXnOO3X777WratKnefPNNH0bmG8773LreIwWHmDtYTbX0z9cD8j63RlG5VVdXq6ioSOnp6c6xoKAgpaenq7Cw0IeRNazy8nJJUsuWLX0cifdlZmZq+PDhLv/N/d3q1avVp08fjRw5Um3atFGvXr30pz/9yddheV1aWpoKCgr0xRdfSJI++eQTbdmyRcOGDfNxZD5G5WZKo1gteeLECdXU1Cg6OtplPDo6Wp9//rmPompYDodDU6ZMUf/+/ZWUlOTrcLxq+fLl2r17t3bu3OnrUBrUv/71Ly1cuFDZ2dl67LHHtHPnTk2ePFkhISEaO3asr8PzmpycHFVUVCgxMVHBwcGqqanRnDlzNHr0aF+H5mOeuGZGcoPFZWZmau/evdqyZYuvQ/GqkpISPfjgg1q7dq3CwsJ8HU6Dcjgc6tOnj55++mlJUq9evbR3714tWrTIr5Pb22+/raVLl2rZsmXq1q2b9uzZoylTpiguLs6vvze8q1Ekt9atWys4OFhlZWUu42VlZYqJifFRVA0nKytL7733njZt2mT6XXhWV1RUpGPHjql3797OsZqaGm3atEnz589XVVWVgoODfRih98TGxqpr164uY126dNHf/vY3H0XUMB555BHl5OTozjvvlCR1795dR44cUW5ubmAnN0+0FQO4LdkorrmFhIQoOTlZBQUFzjGHw6GCggKlpqb6MDLvMgxDWVlZWrlypdatW6eEhARfh+R1Q4YM0WeffaY9e/Y4tz59+mj06NHas2eP3yY2Serfv/8Ft3p88cUX6tChg48iahhnzpxRUJDrn6Lg4GA5HIH7h1nSf2/iNrsFqEZRuUlSdna2xo4dqz59+qhfv36aN2+eKisrNX78eF+H5jWZmZlatmyZVq1apYiICJWWlkr68Y2/TZs29XF03hEREXHBNcUrr7xSrVq18vtrjQ899JDS0tL09NNP67e//a127NihJUuWaMmSJb4OzatuvvlmzZkzR+3bt1e3bt308ccf68UXX9Q999zj69DQiDWa5HbHHXfo+PHjmjFjhkpLS9WzZ0+tWbPmgkUm/mThwoWSpMGDB7uM5+Xlady4cQ0fELyqb9++WrlypaZNm6ZZs2YpISFB8+bN8/uFFa+88ooef/xx/eEPf9CxY8cUFxen++67TzNmzPB1aD7GghIzGs19bgAQCJz3uV1zh2fuc/tiBfe5AQDgDxpNWxIAAgqvvDGF5AYAlsQ1NzNoSwIA/A6VGwBYES8rNYXkBgBWxBNKTKEtCQDwO1RuAGBFVG6mkNwAwJKM/2xmjxGYaEsCAPwOlRsAWJIn3qQduG1JKjcAsKLz19zMbl4yZ84cpaWlKTw8XM2bN691n+LiYg0fPlzh4eFq06aNHnnkEZ07d+6ix/33v/+t0aNHy263q3nz5powYYJOnz7tdnwkNwCA26qrqzVy5Eg98MADtf68pqZGw4cPV3V1tbZu3ao33nhD+fn5l3zbw+jRo7Vv3z6tXbvW+ZLme++91+34eCsAAFiI860AHYdJQU3MHcxxVjr8gVffCpCfn68pU6bo5MmTLuMffPCBbrrpJn3zzTfOV5MtWrRIjz76qI4fP66QkAvfeLB//3517dpVO3fuVJ8+fSRJa9as0a9//Wt9/fXXiouLu+y4qNwAwIoc535MTqa2H1uAFRUVLltVVZXXwy8sLFT37t1d3rmZkZGhiooK7du3r845zZs3dyY2SUpPT1dQUJC2b9/u1vlZUAIAFhISEqKYmBiVFq/1yPGaNWum+Ph4l7GZM2fqiSee8Mjx61JaWnrBy6TPfy4tLa1zTps2bVzGrrjiCrVs2bLOOXUhuQGAhYSFhenQoUOqrq72yPEMw5DNZnMZCw0NrXXfnJwczZ0796LH279/vxITEz0SmzeR3ADAYsLCwhQWFtbg5506darGjRt30X06dep0WceKiYnRjh07XMbKysqcP6trzrFjx1zGzp07p3//+991zqkLyQ0AIEmKiopSVFSUR46VmpqqOXPm6NixY85W49q1a2W329W1a9c655w8eVJFRUVKTk6WJK1bt04Oh0MpKSlunZ8FJQAAtxUXF2vPnj0qLi5WTU2N9uzZoz179jjvSbvxxhvVtWtX/e53v9Mnn3yiDz/8UNOnT1dmZqazLbpjxw4lJibq6NGjkqQuXbpo6NChmjhxonbs2KF//OMfysrK0p133unWSklJkgEAgJvGjh17/uGXLtv69eud+xw+fNgYNmyY0bRpU6N169bG1KlTjbNnzzp/vn79ekOScejQIefYd999Z4waNcpo1qyZYbfbjfHjxxunTp1yOz7ucwMA+B3akgAAv0NyAwD4HZIbAMDvkNwAAH6H5AYA8DskNwCA3yG5AQD8DskNAOB3SG4AAL9DcgMA+B2SGwDA7/x/8LMzFgcmjEcAAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_reward_map(test_flag, state_reward_map):\n",
    "    if test_flag:\n",
    "        # Test the reward map\n",
    "        print(\"The reward map:\")\n",
    "        for i in range(10):\n",
    "            print(state_reward_map[i*10:i*10+10])\n",
    "        \n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.imshow(state_reward_map.reshape(10, 10), cmap='cividis', origin='lower')\n",
    "        plt.colorbar()\n",
    "        plt.title('The reward map')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "test_flag = True\n",
    "test_reward_map(test_flag, state_reward_map)\n",
    "\n",
    "# Running this test, we can see that the map is stored upside down, the origin is in the bottom-left corner\n",
    "# But when displaying, we can just set the origin to 'lower' to display it correctly\n",
    "# So there is no need to manually flip the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2117d0fb60ddb46c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T01:40:01.508983Z",
     "start_time": "2024-04-05T01:40:01.504642Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_x_y_to_state(x, y):\n",
    "    return x + y * 10\n",
    "\n",
    "\n",
    "# def flat_map_to_2D_map(flat_map):\n",
    "#     \"\"\"\n",
    "#     Be very careful that the indexes are formulated like this\n",
    "#         [[90 91 92 93 94 95 96 97 98 99],\n",
    "#          [80 81 82 83 84 85 86 87 88 89],\n",
    "#          ...\n",
    "#          [10 11 12 13 14 15 16 17 18 19],\n",
    "#          [00 01 02 03 04 05 06 07 08 09]]\n",
    "#     \n",
    "#     :param flat_map: (100,) or (100,1) array\n",
    "#     :return: 2D array (10,10), for plotting\n",
    "#     \"\"\"\n",
    "#     # Check if the input is a 2D array, reshape to (100,) if necessary\n",
    "#     if len(flat_map.shape) == 2:\n",
    "#         flat_map = flat_map.reshape(-1)\n",
    "#         \n",
    "#     # Convert the flat map to a 2D map\n",
    "#     map_2d = np.zeros((10, 10))\n",
    "#     for i in range(10):\n",
    "#         for j in range(10):\n",
    "#             map_2d[j, i] = flat_map[i + j * 10]\n",
    "#     return map_2d   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "192ffe20ac8515b1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T01:40:02.780136Z",
     "start_time": "2024-04-05T01:40:02.773129Z"
    }
   },
   "outputs": [],
   "source": [
    "# test_flag = False\n",
    "# \n",
    "# \n",
    "# if test_flag:\n",
    "#     # Test flat_map_to_2D_map\n",
    "#     # Try creating a map with the obstacles and the goal represented in 1D index\n",
    "#     # We can first convert the above defined coordinates to 1D index\n",
    "#     obstacle_coords_1D = [map_x_y_to_state(cell[0], cell[1]) for cell in obstacle_coords]\n",
    "#     goal_coords_1D = [map_x_y_to_state(cell[0], cell[1]) for cell in goal_coords]\n",
    "#     origin_1_coords_1D = [map_x_y_to_state(cell[0], cell[1]) for cell in origin_1_coords]\n",
    "#     origin_2_coords_1D = [map_x_y_to_state(cell[0], cell[1]) for cell in origin_2_coords]\n",
    "#     \n",
    "#     # Mark the obstacles, goal, and origin cells\n",
    "#     state_reward_map_1D_test = np.zeros(100)\n",
    "#     for cell in obstacle_coords_1D:\n",
    "#         state_reward_map_1D_test[cell] = -10\n",
    "#     for cell in goal_coords_1D:\n",
    "#         state_reward_map_1D_test[cell] = 10\n",
    "#     for cell in origin_1_coords_1D:\n",
    "#         state_reward_map_1D_test[cell] = 5\n",
    "#     for cell in origin_2_coords_1D:\n",
    "#         state_reward_map_1D_test[cell] = -5\n",
    "#         \n",
    "#     map_2d = state_reward_map_1D_test.reshape(10, 10)\n",
    "#     \n",
    "#     plt.figure(figsize=(5, 5))\n",
    "#     plt.imshow(map_2d, cmap='cividis', origin='lower')\n",
    "#     plt.colorbar()\n",
    "#     plt.title('Obs, goal, and origins highlighted')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5286e60edf21570b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T01:40:03.562126Z",
     "start_time": "2024-04-05T01:40:03.546079Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the action list\n",
    "actions = {'stay': -1, 'N': 0, 'E': 1, 'S': 2, 'W': 3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "603a3d9d6a0c696e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T01:40:05.923346Z",
     "start_time": "2024-04-05T01:40:05.916992Z"
    }
   },
   "outputs": [],
   "source": [
    "# About the action rewards: \n",
    "# The robot is allowed to stay in the goal state indefinitely, and this stay action gets no reward/cost\n",
    "# When in obstacle cells, the robot is forced to stay with receiving reward -10, but no additional action cost\n",
    "# Taking actions at all other states (including stay), regardless of the outcome, receives a reward of -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e278a1169d15ae4f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T01:40:08.265042Z",
     "start_time": "2024-04-05T01:40:08.241656Z"
    }
   },
   "outputs": [],
   "source": [
    "# About the action probabilities: \n",
    "# The agent moves in the intended direction with probability 0.7\n",
    "# The agent moves in the left/right of the intended direction with probability 0.1\n",
    "# The agent stays in the same cell with probability 0.1\n",
    "# The agent won't move to the opposite direction of the intended direction\n",
    "\n",
    "# Define a function to always make the policy matrix valid\n",
    "def make_policy_valid(policy):\n",
    "    \"\"\"\n",
    "    Make the policy matrix valid\n",
    "    :param policy: a 10x10 matrix, each element is an action, the action given by the policy only depend on the state (grid)\n",
    "    :return: a valid policy matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the policy matrix is 10x10\n",
    "    if policy.shape != (10, 10): raise ValueError('The policy matrix should be a 10x10 matrix')\n",
    "    # Each element should be an integer in [-1, 3]\n",
    "    if not np.all(np.isin(policy, [-1, 0, 1, 2, 3])): raise ValueError('Each element in the policy matrix should be an integer in [-1, 3]')\n",
    "    # Obstacle cells and goal cells should have no action\n",
    "    obstacle_coords = [[9, 9], [8, 9], [7, 9], [6, 9], [5, 9], [4, 9], [3, 9], [2, 9], [1, 9], [0, 9], [9, 8], [9, 7], [9, 6], [9, 5], [9, 4], [9, 3], [9, 2], [9, 1], [9, 0], [0, 0], [1, 0], [2, 0], [3, 0], [4, 0], [5, 0], [6, 0], [7, 0], [8, 0], [9, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [0, 9], [3, 2], [4, 2], [5, 2], [6, 2], [4, 4], [4, 5], [4, 6], [4, 7], [5, 7], [7, 4], [7, 5]]\n",
    "    goal_coords = [[8, 1]]\n",
    "    for cell in obstacle_coords + goal_coords:\n",
    "        policy[cell[1], cell[0]] = -1\n",
    "        \n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12dff71a743fa636",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T01:40:13.717224Z",
     "start_time": "2024-04-05T01:40:13.700159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the transition matrix\n",
    "# The transition matrix is a 100x100 matrix\n",
    "# Each row corresponds to a state\n",
    "# Each column corresponds to a possible next state\n",
    "# The element at [row, column] is the probability of transitioning from state \"row\" to state \"column\" with the intended action\n",
    "# The transition matrix is sparse (mostly zeros)\n",
    "\n",
    "# Compute the transition matrix depending on the policy\n",
    "def get_transition_matrix(policy):\n",
    "    \"\"\"\n",
    "    :param replacement: True uses the new function, otherwise uses the old function to compute the transition matrix\n",
    "    :param policy: (10,10) matrix, each element is an integer from -1 to 3, representing the action to take\n",
    "    :return: (100,100) matrix, representing the probability of moving from state i to state j\n",
    "\n",
    "    state i is at [i//10, i%10]\n",
    "    \"\"\"\n",
    "    # Check if the policy matrix is valid: \n",
    "    # 10x10\n",
    "    if policy.shape != (10, 10):\n",
    "        raise ValueError('The policy matrix should be a 10x10 matrix')\n",
    "    # Each element should be an integer in [-1, 3]\n",
    "    if not np.all(np.isin(policy, [-1, 0, 1, 2, 3])):\n",
    "        raise ValueError('Each element in the policy matrix should be an integer in [-1, 3]')\n",
    "    # Obstacle cells and goal cells should have no action\n",
    "    obstacle_coords = [[9, 9], [8, 9], [7, 9], [6, 9], [5, 9], [4, 9], [3, 9], [2, 9], [1, 9], [0, 9], [9, 8], [9, 7], [9, 6], [9, 5], [9, 4], [9, 3], [9, 2], [9, 1], [9, 0], [0, 0], [1, 0], [2, 0], [3, 0], [4, 0], [5, 0], [6, 0], [7, 0], [8, 0], [9, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [0, 9], [3, 2], [4, 2], [5, 2], [6, 2], [4, 4], [4, 5], [4, 6], [4, 7], [5, 7], [7, 4], [7, 5]]\n",
    "    goal_coords = [[8, 1]]\n",
    "    for cell in obstacle_coords + goal_coords:\n",
    "        if np.abs(policy[cell[1], cell[0]] + 1) > 1e-2:\n",
    "            make_policy_valid(policy)\n",
    "            # raise ValueError('Obstacle cells and goal cells should have no action')\n",
    "    \n",
    "    grid_len = 10\n",
    "    move_prob = 0.7\n",
    "    move_p1_prob = 0.1\n",
    "    move_m1_prob = 0.1\n",
    "    stay_prob = 0.1\n",
    "    \n",
    "    T = np.zeros((grid_len ** 2, grid_len ** 2), dtype=float)\n",
    "    for i in range(grid_len):  # x\n",
    "        for j in range(grid_len):  # y\n",
    "            state = map_x_y_to_state(i, j)  # state = i*10+j, at (i,j)\n",
    "            policy_action = policy[i, j]  # action to take at (i,j) according to policy\n",
    "            if policy_action == 0:  # intend to move N\n",
    "                # (i,j+1) with prob move_prob if not hit N boundary\n",
    "                # (i+1,j) with prob move_p1_prob if not hit E boundary\n",
    "                # (i-1,j) with prob move_m1_prob if not hit W boundary\n",
    "                # (i,j) with prob stay_prob (not hit any boundary), or stay_prob+move_prob (hit N boundary), or stay_prob+move_p1_prob (hit E boundary), or stay_prob+move_m1_prob (hit W boundary)\n",
    "                if j < grid_len - 1:\n",
    "                    T[state, map_x_y_to_state(i, j + 1)] += move_prob\n",
    "                else:\n",
    "                    T[state, state] += move_prob\n",
    "                if i < grid_len - 1:\n",
    "                    T[state, map_x_y_to_state(i + 1, j)] += move_p1_prob\n",
    "                else:\n",
    "                    T[state, state] += move_p1_prob\n",
    "                if i > 0:\n",
    "                    T[state, map_x_y_to_state(i - 1, j)] += move_m1_prob\n",
    "                else:\n",
    "                    T[state, state] += move_m1_prob\n",
    "                T[state, state] += stay_prob\n",
    "\n",
    "            elif policy_action == 1:  # intend to move E\n",
    "                # (i+1,j) with prob move_prob if not hit E boundary\n",
    "                # (i,j-1) with prob move_p1_prob if not hit S boundary\n",
    "                # (i,j+1) with prob move_m1_prob if not hit N boundary\n",
    "                # (i,j) with prob stay_prob (not hit any boundary), or stay_prob+move_prob (hit E boundary), or stay_prob+move_p1_prob (hit S boundary), or stay_prob+move_m1_prob (hit N boundary)\n",
    "                if i < grid_len - 1:\n",
    "                    T[state, map_x_y_to_state(i + 1, j)] += move_prob\n",
    "                else:\n",
    "                    T[state, state] += move_prob\n",
    "                if j > 0:\n",
    "                    T[state, map_x_y_to_state(i, j - 1)] += move_p1_prob\n",
    "                else:\n",
    "                    T[state, state] += move_p1_prob\n",
    "                if j < grid_len - 1:\n",
    "                    T[state, map_x_y_to_state(i, j + 1)] += move_m1_prob\n",
    "                else:\n",
    "                    T[state, state] += move_m1_prob\n",
    "                T[state, state] += stay_prob\n",
    "\n",
    "            elif policy_action == 2:  # intend to move S\n",
    "                # (i,j-1) with prob move_prob if not hit S boundary\n",
    "                # (i-1,j) with prob move_p1_prob if not hit W boundary\n",
    "                # (i+1,j) with prob move_m1_prob if not hit E boundary\n",
    "                # (i,j) with prob stay_prob (not hit any boundary), or stay_prob+move_prob (hit S boundary), or stay_prob+move_p1_prob (hit W boundary), or stay_prob+move_m1_prob (hit E boundary)\n",
    "                if j > 0:\n",
    "                    T[state, map_x_y_to_state(i, j - 1)] += move_prob\n",
    "                else:\n",
    "                    T[state, state] += move_prob\n",
    "                if i > 0:\n",
    "                    T[state, map_x_y_to_state(i - 1, j)] += move_p1_prob\n",
    "                else:\n",
    "                    T[state, state] += move_p1_prob\n",
    "                if i < grid_len - 1:\n",
    "                    T[state, map_x_y_to_state(i + 1, j)] += move_m1_prob\n",
    "                else:\n",
    "                    T[state, state] += move_m1_prob\n",
    "                T[state, state] += stay_prob\n",
    "\n",
    "            elif policy_action == 3:  # intend to move W\n",
    "                # (i-1,j) with prob move_prob if not hit W boundary\n",
    "                # (i,j+1) with prob move_p1_prob if not hit N boundary\n",
    "                # (i,j-1) with prob move_m1_prob if not hit S boundary\n",
    "                # (i,j) with prob stay_prob (not hit any boundary), or stay_prob+move_prob (hit W boundary), or stay_prob+move_p1_prob (hit N boundary), or stay_prob+move_m1_prob (hit S boundary)\n",
    "                if i > 0:\n",
    "                    T[state, map_x_y_to_state(i - 1, j)] += move_prob\n",
    "                else:\n",
    "                    T[state, state] += move_prob\n",
    "                if j < grid_len - 1:\n",
    "                    T[state, map_x_y_to_state(i, j + 1)] += move_p1_prob\n",
    "                else:\n",
    "                    T[state, state] += move_p1_prob\n",
    "                if j > 0:\n",
    "                    T[state, map_x_y_to_state(i, j - 1)] += move_m1_prob\n",
    "                else:\n",
    "                    T[state, state] += move_m1_prob\n",
    "                T[state, state] += stay_prob\n",
    "\n",
    "            else:  # -1 and otherwise, intend to stay\n",
    "                # (i,j) with prob stay_prob\n",
    "                T[state, state] += 1\n",
    "\n",
    "    # Check if the transition matrix is valid\n",
    "    if not np.allclose(np.sum(T, axis=1), 1):\n",
    "        raise ValueError('The transition matrix is not valid')\n",
    "\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c4df2bc54175b20",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T01:40:15.071033Z",
     "start_time": "2024-04-05T01:40:15.060263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy: \n",
      "[[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      " [-1  1  1  1  1  1  1  1 -1 -1]\n",
      " [-1  1  1 -1 -1 -1 -1  1  1 -1]\n",
      " [-1  1  1  1  1  1  1  1  1 -1]\n",
      " [-1  1  1  1 -1  1  1 -1  1 -1]\n",
      " [-1  1  1  1 -1  1  1 -1  1 -1]\n",
      " [-1  1  1  1 -1  1  1  1  1 -1]\n",
      " [-1  1  1  1 -1 -1  1  1  1 -1]\n",
      " [-1  1  1  1  1  1  1  1  1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1]]\n",
      "policy at (3,4): \n",
      "1\n",
      "transition_matrix: \n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "test_flag = True\n",
    "\n",
    "def test_transition_matrix(test_flag):\n",
    "    if test_flag:\n",
    "        # Test get_transition_matrix\n",
    "        # Initialize the policy where the agent always moves E, i.e., policy[x, y] = 1 for all non-obstacle/goal cells\n",
    "        policy = 1 * np.ones((10, 10), dtype=int)\n",
    "        policy = make_policy_valid(policy)\n",
    "        print(\"policy: \")\n",
    "        print(policy)\n",
    "        print(\"policy at (3,4): \")\n",
    "        print(policy[3,4])\n",
    "        transition_matrix = get_transition_matrix(policy)\n",
    "        print(\"transition_matrix: \")\n",
    "        print(transition_matrix)\n",
    "    \n",
    "\n",
    "test_transition_matrix(test_flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6aa57847f23cf54",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T01:40:24.026518Z",
     "start_time": "2024-04-05T01:40:24.010785Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_action_reward_map():\n",
    "    \"\"\"\n",
    "    :return: (100,100) matrix, representing the reward of moving from state i to state j, same dimension as the transition matrix\n",
    "\n",
    "    State i is at [i//10, i%10]\n",
    "    \n",
    "    All actions receive a reward of -1, except for staying at the goal state & obstacles, which receives a reward of 0\n",
    "    \"\"\"\n",
    "    grid_len = 10\n",
    "    action_reward = -np.ones((grid_len ** 2, grid_len ** 2), dtype=float)\n",
    "    for i in range(grid_len):  # x\n",
    "        for j in range(grid_len):  # y\n",
    "            if [i, j] in obstacle_coords:  # obstacle, cost 0 to stay\n",
    "                state = map_x_y_to_state(i, j)\n",
    "                action_reward[state, state] = 0\n",
    "            elif [i, j] in goal_coords:  # goal, cost 0 to stay\n",
    "                state = map_x_y_to_state(i, j)\n",
    "                action_reward[state, state] = 0\n",
    "            else:  # other states, cost 1 to move to other states\n",
    "                # since we initialize the action cost to 1, we don't need to do anything here\n",
    "                pass\n",
    "    return action_reward\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68a1b284b9f4e67a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T01:40:29.212791Z",
     "start_time": "2024-04-05T01:40:29.193603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -1. -1. ... -1. -1. -1.]\n",
      " [-1.  0. -1. ... -1. -1. -1.]\n",
      " [-1. -1.  0. ... -1. -1. -1.]\n",
      " ...\n",
      " [-1. -1. -1. ...  0. -1. -1.]\n",
      " [-1. -1. -1. ... -1.  0. -1.]\n",
      " [-1. -1. -1. ... -1. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "action_reward_map = get_action_reward_map()\n",
    "print(action_reward_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0e473f9fe72b0c2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T02:05:21.264648Z",
     "start_time": "2024-04-05T02:03:54.881136Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 54\u001B[0m\n\u001B[0;32m     52\u001B[0m policy \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mones((\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m10\u001B[39m), dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mint\u001B[39m)\n\u001B[0;32m     53\u001B[0m policy \u001B[38;5;241m=\u001B[39m make_policy_valid(policy)\n\u001B[1;32m---> 54\u001B[0m V \u001B[38;5;241m=\u001B[39m \u001B[43mpolicy_evaluation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpolicy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_reward_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction_reward_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28mprint\u001B[39m(V[map_x_y_to_state(\u001B[38;5;241m1\u001B[39m,\u001B[38;5;241m3\u001B[39m)], V[map_x_y_to_state(\u001B[38;5;241m3\u001B[39m,\u001B[38;5;241m1\u001B[39m)], V[map_x_y_to_state(\u001B[38;5;241m8\u001B[39m,\u001B[38;5;241m1\u001B[39m)])\n\u001B[0;32m     57\u001B[0m \u001B[38;5;66;03m# Convert V to a 10x10 matrix for visualization\u001B[39;00m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;66;03m# Be careful that the origin is in the bottom-left corner\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[23], line 41\u001B[0m, in \u001B[0;36mpolicy_evaluation\u001B[1;34m(policy, state_reward_map, action_reward_map, gamma, theta)\u001B[0m\n\u001B[0;32m     39\u001B[0m     V \u001B[38;5;241m=\u001B[39m V \u001B[38;5;241m/\u001B[39m (iteration \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     40\u001B[0m     V_vis \u001B[38;5;241m=\u001B[39m V\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m---> 41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241m.\u001B[39mmax(np\u001B[38;5;241m.\u001B[39mabs(V \u001B[38;5;241m-\u001B[39m V_prev)) \u001B[38;5;241m<\u001B[39m theta:\n\u001B[0;32m     42\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m# print(f'Policy evaluated in {iteration} iterations')\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[23], line 41\u001B[0m, in \u001B[0;36mpolicy_evaluation\u001B[1;34m(policy, state_reward_map, action_reward_map, gamma, theta)\u001B[0m\n\u001B[0;32m     39\u001B[0m     V \u001B[38;5;241m=\u001B[39m V \u001B[38;5;241m/\u001B[39m (iteration \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     40\u001B[0m     V_vis \u001B[38;5;241m=\u001B[39m V\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m---> 41\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241m.\u001B[39mmax(np\u001B[38;5;241m.\u001B[39mabs(V \u001B[38;5;241m-\u001B[39m V_prev)) \u001B[38;5;241m<\u001B[39m theta:\n\u001B[0;32m     42\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m# print(f'Policy evaluated in {iteration} iterations')\u001B[39;00m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx:937\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx:928\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mD:\\Program Files\\JetBrains\\PyCharm 2023.3.4\\plugins\\python\\helpers\\pydev\\pydevd.py:1184\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1181\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1183\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1184\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Program Files\\JetBrains\\PyCharm 2023.3.4\\plugins\\python\\helpers\\pydev\\pydevd.py:1199\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1196\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1198\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1199\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1203\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def policy_evaluation(policy, state_reward_map, action_reward_map, gamma=0.9, theta=1e-5):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [(10,10) numpy array] The policy to evaluate, where policy[x, y] gives the action to take at position (x, y).\n",
    "        transition_matrix: [(100,100) numpy array] State transition probabilities for each action.\n",
    "        state_reward_map: [(100,) numpy array] Immediate rewards for all states.\n",
    "        action_cost_map: [(100,100) numpy array] The cost of taking an action from state i to state j. NO LONGER NEEDED\n",
    "        gamma: float, Discount factor.\n",
    "        theta: float, A threshold of change for the value function to determine convergence.\n",
    "        \n",
    "    Returns:\n",
    "        V: (100,) numpy array, The value for each state under the specified policy.\n",
    "    \"\"\"\n",
    "    V = np.zeros(100)  # Initialize state-value function with zeros for each state\n",
    "    V_vis = V.reshape(10, 10)\n",
    "\n",
    "    obstacle_coords = [[9, 9], [8, 9], [7, 9], [6, 9], [5, 9], [4, 9], [3, 9], [2, 9], [1, 9], [0, 9], [9, 8], [9, 7],\n",
    "                       [9, 6], [9, 5], [9, 4], [9, 3], [9, 2], [9, 1], [9, 0], [0, 0], [1, 0], [2, 0], [3, 0], [4, 0],\n",
    "                       [5, 0], [6, 0], [7, 0], [8, 0], [9, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7],\n",
    "                       [0, 8], [0, 9], [3, 2], [4, 2], [5, 2], [6, 2], [4, 4], [4, 5], [4, 6], [4, 7], [5, 7], [7, 4],\n",
    "                       [7, 5]]\n",
    "    goal_coords = [[8, 1]]\n",
    "    obstacle_states = [map_x_y_to_state(cell[0], cell[1]) for cell in obstacle_coords]\n",
    "    goal_states = [map_x_y_to_state(cell[0], cell[1]) for cell in goal_coords]\n",
    "    \n",
    "    transition_matrix = get_transition_matrix(policy)\n",
    "\n",
    "    for iteration in range(1000):\n",
    "        V_prev = deepcopy(V)\n",
    "        for s in range(100):\n",
    "            # Skip the obstacle and goal states\n",
    "            if s in obstacle_states or s in goal_states:\n",
    "                V[s] += state_reward_map[s]\n",
    "                continue\n",
    "            for s_prime in range(100):\n",
    "                V[s] += transition_matrix[s, s_prime] * (state_reward_map[s_prime] + action_reward_map[s, s_prime] + gamma * V_prev[s_prime])\n",
    "        V = V / (iteration + 1)\n",
    "        V_vis = V.reshape(10, 10)\n",
    "        if np.max(np.abs(V - V_prev)) < theta:\n",
    "            break\n",
    "\n",
    "    # print(f'Policy evaluated in {iteration} iterations')\n",
    "    return V\n",
    "    \n",
    "\n",
    "test_flag = True\n",
    "if test_flag:\n",
    "    # Test policy_evaluation\n",
    "    # Initialize a static policy\n",
    "    policy = 1 * np.ones((10, 10), dtype=int)\n",
    "    policy = make_policy_valid(policy)\n",
    "    V = policy_evaluation(policy, state_reward_map, action_reward_map)\n",
    "    print(V[map_x_y_to_state(1,3)], V[map_x_y_to_state(3,1)], V[map_x_y_to_state(8,1)])\n",
    "    \n",
    "    # Convert V to a 10x10 matrix for visualization\n",
    "    # Be careful that the origin is in the bottom-left corner\n",
    "    V_2D = V.reshape(10, 10)\n",
    "    print(\"V-2D: \")\n",
    "    np.set_printoptions(precision=2)\n",
    "    print(V_2D)\n",
    "    \n",
    "    # Visualize the value function\n",
    "    # V[map_x_y_to_state(8, 1)] = 100  # Use this to indentify the goal state\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(V_2D, cmap='cividis', origin='lower')\n",
    "    plt.colorbar()\n",
    "    plt.title('Value function for Given Policy')\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks(range(10))\n",
    "    plt.xlabel('X axis')\n",
    "    plt.ylabel('Y axis')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "664dc700469b8131",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T01:39:01.795074Z",
     "start_time": "2024-04-05T01:39:01.782249Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "50c552cecec06e57",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T01:39:01.810194Z",
     "start_time": "2024-04-05T01:39:01.797178Z"
    }
   },
   "outputs": [],
   "source": [
    "def policy_improvement(V, policy, state_reward_map, action_cost_map, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Improve a policy given the value function.\n",
    "    \n",
    "    Args:\n",
    "        V: [(100,) numpy array] The value for each state under the specified policy.\n",
    "        policy: [(10,10) numpy array] The policy to evaluate, where policy[x, y] gives the action to take at position (x, y).\n",
    "        transition_matrix: [(100,100) numpy array] State transition probabilities for each action.\n",
    "        state_reward_map: [(100,) numpy array] Immediate rewards for all states.\n",
    "        action_cost_map: [(100,100) numpy array] The cost of taking an action from state i to state j.\n",
    "        gamma: float, Discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        policy: [10x10 numpy array] The improved policy.\n",
    "        policy_stable: [bool] Whether the policy is stable.\n",
    "    \"\"\"\n",
    "    obstacle_coords = [[9, 9], [8, 9], [7, 9], [6, 9], [5, 9], [4, 9], [3, 9], [2, 9], [1, 9], [0, 9],\n",
    "                  [9, 8], [9, 7], [9, 6], [9, 5], [9, 4], [9, 3], [9, 2], [9, 1], [9, 0],\n",
    "                  [0, 0], [1, 0], [2, 0], [3, 0], [4, 0], [5, 0], [6, 0], [7, 0], [8, 0], [9, 0],\n",
    "                  [0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [0, 9],\n",
    "                  [3, 2], [4, 2], [5, 2], [6, 2],\n",
    "                  [4, 4], [4, 5], [4, 6], [4, 7], [5, 7],\n",
    "                  [7, 4], [7, 5]]\n",
    "    goal_coords = [[8, 1]]\n",
    "    \n",
    "    obstacle_states = [map_x_y_to_state(cell[0], cell[1]) for cell in obstacle_coords]\n",
    "    goal_states = [map_x_y_to_state(cell[0], cell[1]) for cell in goal_coords]    \n",
    "    \n",
    "    transition_matrix = get_transition_matrix(policy)\n",
    "    \n",
    "    policy_stable = True  # Initialize policy_stable to True\n",
    "\n",
    "    ## For debugging\n",
    "    V_visual = V.reshape(10, 10)\n",
    "    \n",
    "    for s in range(100):\n",
    "        # Skip the obstacle and goal states\n",
    "        if s in obstacle_states or s in goal_states:\n",
    "            policy[s % 10, s // 10] = -1\n",
    "            continue\n",
    "        \n",
    "        # For all normal states\n",
    "        # Convert state index to (x, y) coordinate\n",
    "        x, y = s % 10, s // 10\n",
    "        \n",
    "        # Initialize a variable to keep track of the best action value\n",
    "        best_action_value = -np.inf\n",
    "        best_action = None\n",
    "        \n",
    "        # Compare all possible actions to find the best one\n",
    "        # a = -1 (stay)\n",
    "        action_value = 0\n",
    "        for s_prime in range(100):\n",
    "            action_value += transition_matrix[s, s_prime] * (state_reward_map[s_prime] + action_cost_map[s, s_prime] + gamma * V[s_prime])\n",
    "        if action_value > best_action_value:\n",
    "            best_action_value = action_value\n",
    "            best_action = -1\n",
    "\n",
    "        # a = 0 (N)\n",
    "        action_value = 0\n",
    "        if y < 9:\n",
    "            s_prime = s + 10\n",
    "            action_value += transition_matrix[s, s_prime] * (state_reward_map[s_prime] + action_cost_map[s, s_prime] + gamma * V[s_prime])\n",
    "        else:\n",
    "            action_value += transition_matrix[s, s] * (state_reward_map[s] + action_cost_map[s, s] + gamma * V[s])\n",
    "        if action_value > best_action_value:\n",
    "            best_action_value = action_value\n",
    "            best_action = 0\n",
    "\n",
    "        # a = 1 (E)\n",
    "        action_value = 0\n",
    "        if x < 9:\n",
    "            s_prime = s + 1\n",
    "            action_value += transition_matrix[s, s_prime] * (state_reward_map[s_prime] + action_cost_map[s, s_prime] + gamma * V[s_prime])\n",
    "        else:\n",
    "            action_value += transition_matrix[s, s] * (state_reward_map[s] + action_cost_map[s, s] + gamma * V[s])\n",
    "        if action_value > best_action_value:\n",
    "            best_action_value = action_value\n",
    "            best_action = 1\n",
    "\n",
    "        # a = 2 (S)\n",
    "        action_value = 0\n",
    "        if y > 0:\n",
    "            s_prime = s - 10\n",
    "            action_value += transition_matrix[s, s_prime] * (state_reward_map[s_prime] + action_cost_map[s, s_prime] + gamma * V[s_prime])\n",
    "        else:\n",
    "            action_value += transition_matrix[s, s] * (state_reward_map[s] + action_cost_map[s, s] + gamma * V[s])\n",
    "        if action_value > best_action_value:\n",
    "            best_action_value = action_value\n",
    "            best_action = 2\n",
    "\n",
    "        # a = 3 (W)\n",
    "        action_value = 0\n",
    "        if x > 0:\n",
    "            s_prime = s - 1\n",
    "            action_value += transition_matrix[s, s_prime] * (state_reward_map[s_prime] + action_cost_map[s, s_prime] + gamma * V[s_prime])\n",
    "        else:\n",
    "            action_value += transition_matrix[s, s] * (state_reward_map[s] + action_cost_map[s, s] + gamma * V[s])\n",
    "        if action_value > best_action_value:\n",
    "            best_action_value = action_value\n",
    "            best_action = 3\n",
    "\n",
    "        # Check if the found best action differs from the current policy\n",
    "        current_action = policy[x, y]\n",
    "        if best_action != current_action:\n",
    "            policy_stable = False\n",
    "            policy[x, y] = best_action  # Update the policy with the new best action\n",
    "\n",
    "    return policy, policy_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a5614b16da21f56e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T01:39:01.839279Z",
     "start_time": "2024-04-05T01:39:01.811298Z"
    }
   },
   "outputs": [],
   "source": [
    "def policy_iteration(policy, state_reward_map, action_cost_map, gamma=0.9, theta=1e-4, save_for_vis=False, max_iter=100):\n",
    "    \"\"\"\n",
    "    Perform policy iteration to find the optimal policy.\n",
    "    \n",
    "    Args:\n",
    "        policy: [10x10 numpy array] The initial policy to evaluate.\n",
    "        state_reward_map: [numpy array] Immediate rewards for all states.\n",
    "        gamma: float, Discount factor.\n",
    "        theta: float, A threshold of change for the value function to determine convergence.\n",
    "        \n",
    "    Returns:\n",
    "        policy: [10x10 numpy array] The optimal policy.\n",
    "    \"\"\"\n",
    "    policy_stable = False  # Initialize policy_stable to False\n",
    "    \n",
    "    # We want to save the evaluation and improvement results in the first 5 iterations and the last iteration\n",
    "    # This is to visualize the policy improvement process\n",
    "    V_list = []\n",
    "    policy_list = []\n",
    "    iteration = 0\n",
    "    while iteration < max_iter:\n",
    "        \n",
    "        if iteration % 10 == 0:\n",
    "            print(f'Policy iteration in progress, iteration {iteration}')\n",
    "        \n",
    "        iteration += 1\n",
    "        V = policy_evaluation(policy, state_reward_map, action_cost_map, gamma, theta)\n",
    "        policy, policy_stable = policy_improvement(V, policy, state_reward_map, action_cost_map, gamma)\n",
    "        # policy = make_policy_valid(policy)\n",
    "        if iteration <= 5 or policy_stable:\n",
    "            V_list.append(V)\n",
    "            policy_list.append(policy)\n",
    "        if policy_stable:\n",
    "            break\n",
    "            \n",
    "    print(f'Policy iteration converged in {iteration} iterations')\n",
    "    \n",
    "    if save_for_vis:\n",
    "        return policy, policy_stable, V_list, policy_list\n",
    "    else:\n",
    "        return policy, policy_stable\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "58b3d17ff9d7af1d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T01:39:06.158256Z",
     "start_time": "2024-04-05T01:39:01.839279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy iteration in progress, iteration 0\n",
      "Policy iteration in progress, iteration 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[92], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m policy \u001B[38;5;241m=\u001B[39m make_policy_valid(policy)\n\u001B[0;32m      5\u001B[0m transition_matrix \u001B[38;5;241m=\u001B[39m get_transition_matrix(policy)\n\u001B[1;32m----> 6\u001B[0m policy, convergence \u001B[38;5;241m=\u001B[39m \u001B[43mpolicy_iteration\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpolicy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_reward_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction_reward_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConverged =\u001B[39m\u001B[38;5;124m\"\u001B[39m, convergence)\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(policy)\n",
      "Cell \u001B[1;32mIn[91], line 28\u001B[0m, in \u001B[0;36mpolicy_iteration\u001B[1;34m(policy, state_reward_map, action_cost_map, gamma, theta, save_for_vis, max_iter)\u001B[0m\n\u001B[0;32m     26\u001B[0m iteration \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     27\u001B[0m V \u001B[38;5;241m=\u001B[39m policy_evaluation(policy, state_reward_map, action_cost_map, gamma, theta)\n\u001B[1;32m---> 28\u001B[0m policy, policy_stable \u001B[38;5;241m=\u001B[39m \u001B[43mpolicy_improvement\u001B[49m\u001B[43m(\u001B[49m\u001B[43mV\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpolicy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate_reward_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction_cost_map\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# policy = make_policy_valid(policy)\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m iteration \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m policy_stable:\n",
      "Cell \u001B[1;32mIn[90], line 29\u001B[0m, in \u001B[0;36mpolicy_improvement\u001B[1;34m(V, policy, state_reward_map, action_cost_map, gamma)\u001B[0m\n\u001B[0;32m     26\u001B[0m obstacle_states \u001B[38;5;241m=\u001B[39m [map_x_y_to_state(cell[\u001B[38;5;241m0\u001B[39m], cell[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m cell \u001B[38;5;129;01min\u001B[39;00m obstacle_coords]\n\u001B[0;32m     27\u001B[0m goal_states \u001B[38;5;241m=\u001B[39m [map_x_y_to_state(cell[\u001B[38;5;241m0\u001B[39m], cell[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;28;01mfor\u001B[39;00m cell \u001B[38;5;129;01min\u001B[39;00m goal_coords]    \n\u001B[1;32m---> 29\u001B[0m transition_matrix \u001B[38;5;241m=\u001B[39m \u001B[43mget_transition_matrix\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpolicy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m policy_stable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m  \u001B[38;5;66;03m# Initialize policy_stable to True\u001B[39;00m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m## For debugging\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[85], line 38\u001B[0m, in \u001B[0;36mget_transition_matrix\u001B[1;34m(policy)\u001B[0m\n\u001B[0;32m     35\u001B[0m move_m1_prob \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.1\u001B[39m\n\u001B[0;32m     36\u001B[0m stay_prob \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.1\u001B[39m\n\u001B[1;32m---> 38\u001B[0m T \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgrid_len\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrid_len\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(grid_len):  \u001B[38;5;66;03m# x\u001B[39;00m\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(grid_len):  \u001B[38;5;66;03m# y\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Test policy_iteration\n",
    "# Initialize the policy where the agent always moves N, i.e., policy[x, y] = 1 for all non-obstacle/goal cells\n",
    "policy = 1 * np.ones((10, 10), dtype=int)\n",
    "policy = make_policy_valid(policy)\n",
    "transition_matrix = get_transition_matrix(policy)\n",
    "policy, convergence = policy_iteration(policy, state_reward_map, action_reward_map)\n",
    "print(\"Converged =\", convergence)\n",
    "print(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab933fe09f7cdd6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T01:39:06.160516Z",
     "start_time": "2024-04-05T01:39:06.160516Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the policy using arrows\n",
    "# The origin is in the bottom-left corner\n",
    "# The arrows are in the same direction as the action\n",
    "# The arrows are in the center of the cell\n",
    "# No arrow is shown for obstacle cells and goal cells\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(V_2D, cmap='cividis', origin='lower')\n",
    "plt.colorbar()\n",
    "plt.title('Optimal Policy')\n",
    "plt.xticks(range(10))\n",
    "plt.yticks(range(10))\n",
    "plt.xlabel('X axis')\n",
    "plt.ylabel('Y axis')\n",
    "# policy[3, 7] = 3  # Use this to see if the policy is correctly visualized at desired coordinates\n",
    "for y in range(10):\n",
    "    for x in range(10):\n",
    "        if [x, y] in obstacle_coords or [x, y] in goal_coords:\n",
    "            continue\n",
    "        if policy[x, y] == 0:  # N\n",
    "            plt.arrow(x, y, 0, 0.4, head_width=0.1, head_length=0.1, fc='r', ec='r')\n",
    "        elif policy[x, y] == 1:  # E\n",
    "            plt.arrow(x, y, 0.4, 0, head_width=0.1, head_length=0.1, fc='r', ec='r')\n",
    "        elif policy[x, y] == 2:  # S\n",
    "            plt.arrow(x, y, 0, -0.4, head_width=0.1, head_length=0.1, fc='r', ec='r')\n",
    "        elif policy[x, y] == 3:  # W\n",
    "            plt.arrow(x, y, -0.4, 0, head_width=0.1, head_length=0.1, fc='r', ec='r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85f2b37213aa88",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-05T01:39:06.161577Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
